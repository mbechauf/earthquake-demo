{"paragraphs":[{"text":"%sh\n\ncd /tmp\nif [ ! -e /tmp/GEM-GHEC-v1.txt ]\nthen\n\n    wget https://s3.amazonaws.com/bcgta-earthquake-data/GEM-GHEC-v1.txt\n    wget https://s3.amazonaws.com/bcgta-earthquake-data/isc-gem-cat.zip\n    \n    unzip isc-gem-cat.zip isc-gem-cat.csv\n    hadoop fs -put GEM-GHEC-v1.txt /tmp\n    hadoop fs -put isc-gem-cat.csv /tmp\n\nfi\n","user":"anonymous","dateUpdated":"2018-04-07T03:45:45+0000","config":{"colWidth":12,"editorMode":"ace/mode/sh","results":{},"enabled":true,"editorSetting":{"language":"sh","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1523072123341_1715138593","id":"20180406-171241_777364924","dateCreated":"2018-04-07T03:35:23+0000","dateStarted":"2018-04-07T03:45:45+0000","dateFinished":"2018-04-07T03:45:45+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:28329"},{"title":"Make a Spark RDD and a schema to go with it. Perform some more clean up steps of the data.","text":"\n//create RDD\nval historicalData = sc.textFile(\"/tmp/GEM-GHEC-v1.txt\")\n\n//define the schema\nval globalInstrumentalCatalogData = sc.textFile(\"/tmp/isc-gem-cat.csv\")\ncase class EarthQuake(\nid: String,\ndate: String,\nlat: Double,\nlon: Double,\ndepth: Double,\nmag: Double,\nmunc: Double\n)\n\n//clean up the records. Delimit by tabs and peel out the year/month\n//remove unwanted data entries\nval historical = historicalData.filter(s =>\n!s.startsWith(\"#\") && !s.startsWith(\"En\\tSource\")\n).map{s=>\n//make a function to peel out the year\ndef get(s:String) = {\nif (s==\"\") {\n\"00\"\n} else {\ns\n}\n}\nval r = s.split(\"\\t\")\nval year = r(2) // year\nif (r(2) != \"\") {\nval month = r(2)\n} else {\n//...and peel out the month\nval month = \"00\"\n}\n//format the date\nval date = r(2) + \"-\" + get(r(3)) + \"-\" + get(r(4)) + \" \" + get(r(5)) + \":\" + get(r(6)) + \":\" + get(r(7)) + \".00\"\n\n//extract interesting earthquake data:  id, date, depth, magnitude, \"uncorrected\" magnitude\nEarthQuake(\nr(0).toString, // id\ndate,\nget(r(9)).toDouble,\nget(r(10)).toDouble,\nget(r(14)).toDouble, // depth\nget(r(17)).toDouble, // mag\nget(r(18)).toDouble // mag unc\n\n)\n}\n\nval earthQuake = globalInstrumentalCatalogData.filter(!_.startsWith(\"#\")).map{s=>\nval r = s.split(\",\")\nEarthQuake(\nr(23).trim,\nr(0).trim,\nr(1).toDouble,\nr(2).toDouble,\nr(7).toDouble,\nr(10).toDouble,\nr(11).toDouble\n)\n}.union(historical).toDF   //make a dataframe\n//make a table to query\nearthQuake.registerTempTable(\"eq\")","user":"anonymous","dateUpdated":"2018-04-07T03:43:07+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/scala","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1523072123342_1716292839","id":"20180407-020916_542540771","dateCreated":"2018-04-07T03:35:23+0000","dateStarted":"2018-04-07T03:43:07+0000","dateFinished":"2018-04-07T03:43:10+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:28331"},{"title":"Display the average magnitude, maximum magnitude, and minimum magnitude for all the earthquakes per year.","text":"%sql select substring(date, 0, 4) as dt, avg(mag) as avg, max(mag) as max, min(mag) as min from eq group by substring(date, 0, 4) order by dt\n\n","user":"anonymous","dateUpdated":"2018-04-07T03:49:46+0000","config":{"editorSetting":{"language":"sql","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/sql","title":true,"results":{"0":{"graph":{"mode":"scatterChart","height":300,"optionOpen":false},"helium":{}}},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1523072123342_1716292839","id":"20180406-171425_1279942251","dateCreated":"2018-04-07T03:35:23+0000","dateStarted":"2018-04-07T03:43:07+0000","dateFinished":"2018-04-07T03:43:31+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:28332"},{"title":"Make another table with the average depth for all earthquakes that year.","text":"%sql select substring(date, 0, 4) as dt, avg(depth) as depth from eq group by substring(date, 0, 4) order by dt","user":"anonymous","dateUpdated":"2018-04-07T03:43:07+0000","config":{"editorSetting":{"language":"sql","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/sql","title":true,"results":{"0":{"graph":{"mode":"scatterChart","height":300,"optionOpen":false},"helium":{}}},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1523072123342_1716292839","id":"20180407-021104_62089526","dateCreated":"2018-04-07T03:35:23+0000","dateStarted":"2018-04-07T03:43:10+0000","dateFinished":"2018-04-07T03:43:34+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:28333"},{"title":"Plot the number of recorded earthquakes per year","text":"%sql \nselect substring(date, 0, 4) as dt, count(*) as cnt \nfrom eq \nwhere substring(date, 0, 4) > ${dt=1800}\ngroup by substring(date, 0, 4) \norder by dt","user":"anonymous","dateUpdated":"2018-04-07T03:43:07+0000","config":{"editorSetting":{"language":"sql","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/sql","title":true,"results":{"0":{"graph":{"mode":"scatterChart","height":300,"optionOpen":false},"helium":{}}},"enabled":true},"settings":{"params":{"dt":"1800"},"forms":{"dt":{"name":"dt","defaultValue":"1800","hidden":false,"$$hashKey":"object:29028"}}},"apps":[],"jobName":"paragraph_1523072123342_1716292839","id":"20180407-021136_354545145","dateCreated":"2018-04-07T03:35:23+0000","dateStarted":"2018-04-07T03:43:32+0000","dateFinished":"2018-04-07T03:43:37+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:28334"},{"title":"Now lets run a machine learning algorithm to predict … the probabilty of a magnitude 6.0 or higher.","text":"import org.apache.spark.mllib.linalg.Vectors\nimport org.apache.spark.ml.feature.Bucketizer\nimport org.apache.spark.ml.Pipeline\nimport org.apache.spark.ml.feature.PCA\nimport org.apache.spark.ml.classification.LogisticRegression\nimport org.apache.spark.ml.feature.Binarizer","user":"anonymous","dateUpdated":"2018-04-07T03:43:07+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/scala","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1523072123343_1715908090","id":"20180407-021354_1451834076","dateCreated":"2018-04-07T03:35:23+0000","dateStarted":"2018-04-07T03:43:35+0000","dateFinished":"2018-04-07T03:43:39+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:28335"},{"title":"Let's look at a row of the earthquake data and create a dataframe (with year instead of full date).","text":"earthQuake.show(1)\nval eq2 = earthQuake.selectExpr(\"id\", \"substr(date,1,4) as year\", \"lat\", \"lon\", \"depth\", \"mag\", \"munc\").toDF","user":"anonymous","dateUpdated":"2018-04-07T03:43:07+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/scala","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1523072123343_1715908090","id":"20180407-021502_1155789653","dateCreated":"2018-04-07T03:35:23+0000","dateStarted":"2018-04-07T03:43:38+0000","dateFinished":"2018-04-07T03:43:40+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:28336"},{"title":"Some type casts","text":"val df = eq2.select(eq2(\"id\").cast(\"double\"),\neq2(\"year\").cast(\"double\"),\neq2(\"lat\").cast(\"double\"),\neq2(\"lon\").cast(\"double\"),\neq2(\"depth\").cast(\"double\"),\neq2(\"mag\").cast(\"double\"),\neq2(\"munc\").cast(\"double\"))\n\ndf.show(3)\n\n","user":"anonymous","dateUpdated":"2018-04-07T03:43:07+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/scala","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1523072123343_1715908090","id":"20180407-021522_90407659","dateCreated":"2018-04-07T03:35:23+0000","dateStarted":"2018-04-07T03:43:40+0000","dateFinished":"2018-04-07T03:43:41+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:28337"},{"title":"Now create training and testing data sets. (60% for training and 40% for testing)","text":"val Array(training, testing) = df.randomSplit(Array(0.6, 0.4))","user":"anonymous","dateUpdated":"2018-04-07T03:43:08+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/scala","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1523072123343_1715908090","id":"20180407-024931_201577377","dateCreated":"2018-04-07T03:35:23+0000","dateStarted":"2018-04-07T03:43:40+0000","dateFinished":"2018-04-07T03:43:42+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:28338"},{"title":"We'll use a VectorAssembler to combine many features into one feature column (logistic regression likes that)","text":"import org.apache.spark.ml.feature.VectorAssembler\nval assembler = new VectorAssembler()\n  .setInputCols(Array(\"id\", \"year\", \"lat\", \"lon\", \"depth\", \"mag\", \"munc\"))\n  .setOutputCol(\"featureSet\")","user":"anonymous","dateUpdated":"2018-04-07T03:43:08+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/scala","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1523072123344_1726296311","id":"20180407-021603_205356892","dateCreated":"2018-04-07T03:35:23+0000","dateStarted":"2018-04-07T03:43:41+0000","dateFinished":"2018-04-07T03:43:42+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:28339"},{"title":"Set the binary threshold to Richter scale Magnitude of 6.0","text":"val binaryClassifier = new Binarizer().setInputCol(\"mag\").setOutputCol(\"binaryLabel\").setThreshold(6.0)","user":"anonymous","dateUpdated":"2018-04-07T03:43:08+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/scala","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1523072123344_1726296311","id":"20180407-021623_2126084382","dateCreated":"2018-04-07T03:35:23+0000","dateStarted":"2018-04-07T03:43:42+0000","dateFinished":"2018-04-07T03:43:43+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:28340"},{"title":"Create a logistic regression model and set some parameters","text":"val lr = new LogisticRegression().setMaxIter(20).setRegParam(0.2).setElasticNetParam(0.8).setLabelCol(\"binaryLabel\").setFeaturesCol(\"featureSet\")","user":"anonymous","dateUpdated":"2018-04-07T03:43:08+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/scala","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1523072123344_1726296311","id":"20180407-021638_446259416","dateCreated":"2018-04-07T03:35:23+0000","dateStarted":"2018-04-07T03:43:42+0000","dateFinished":"2018-04-07T03:43:43+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:28341"},{"title":"Chain everything together with a Spark Pipeline (great way to organize steps)","text":"val pipeline = new Pipeline()\n  .setStages(Array(assembler, binaryClassifier, lr))","user":"anonymous","dateUpdated":"2018-04-07T03:43:08+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/scala","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1523072123344_1726296311","id":"20180407-021653_34063029","dateCreated":"2018-04-07T03:35:23+0000","dateStarted":"2018-04-07T03:43:43+0000","dateFinished":"2018-04-07T03:43:43+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:28342"},{"title":"Train the model","text":"val model = pipeline.fit(training)","user":"anonymous","dateUpdated":"2018-04-07T03:43:08+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/scala","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1523072123345_1725911562","id":"20180407-021707_1684652937","dateCreated":"2018-04-07T03:35:23+0000","dateStarted":"2018-04-07T03:43:43+0000","dateFinished":"2018-04-07T03:43:56+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:28343"},{"title":"Make predictions","text":"val prediction = model.transform(testing)","user":"anonymous","dateUpdated":"2018-04-07T03:43:08+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/scala","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1523072123345_1725911562","id":"20180407-021718_1069342043","dateCreated":"2018-04-07T03:35:23+0000","dateStarted":"2018-04-07T03:43:44+0000","dateFinished":"2018-04-07T03:43:56+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:28344"},{"title":"Display the results","text":"prediction.select(\"prediction\", \"binaryLabel\", \"mag\").show(10)","user":"anonymous","dateUpdated":"2018-04-07T03:43:08+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/scala","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1523072123345_1725911562","id":"20180407-021735_1271720828","dateCreated":"2018-04-07T03:35:23+0000","dateStarted":"2018-04-07T03:43:57+0000","dateFinished":"2018-04-07T03:43:58+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:28345"},{"title":"How well did we do? We need to quantify the error.","text":"val matches = udf((A : Int, B: Int) => {\n    if (A+B == 1) 0\n    else 1\n})\n\nval total = prediction.count\nval rightWrong = prediction.withColumn(\"matches\", matches($\"prediction\", $\"binaryLabel\")).groupBy(\"matches\").count.toDF\nrightWrong.registerTempTable(\"rightWrong\")\nrightWrong.show","user":"anonymous","dateUpdated":"2018-04-07T03:43:08+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/scala","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1523072123345_1725911562","id":"20180407-021747_1811636731","dateCreated":"2018-04-07T03:35:23+0000","dateStarted":"2018-04-07T03:43:57+0000","dateFinished":"2018-04-07T03:44:04+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:28346"},{"title":"Accuracy = Correct / Total","text":"rightWrong.where($\"matches\"===1).select(rightWrong(\"count\") / total* 100 ).show","user":"anonymous","dateUpdated":"2018-04-07T03:43:08+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/scala","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1523072123346_1727065809","id":"20180407-021806_1469423512","dateCreated":"2018-04-07T03:35:23+0000","dateStarted":"2018-04-07T03:43:59+0000","dateFinished":"2018-04-07T03:44:08+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:28347"},{"title":"Not too bad an accuracy !","user":"anonymous","dateUpdated":"2018-04-07T03:43:08+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/scala","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1523072123346_1727065809","id":"20180407-021820_463237969","dateCreated":"2018-04-07T03:35:23+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:28348"}],"name":"Earthquake Test","id":"2DCRR35N5","angularObjects":{"2BRWU4WXC:shared_process":[],"2AM1YV5CU:shared_process":[],"2AJXGMUUJ:shared_process":[],"2ANGGHHMQ:shared_process":[],"2AKK3QQXU:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}